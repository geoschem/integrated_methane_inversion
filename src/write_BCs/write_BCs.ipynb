{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"../inversion_scripts/operators/\")\n",
    "sys.path.insert(1, \"../inversion_scripts\")\n",
    "\n",
    "import yaml\n",
    "with open(\"config_write_BCs.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "from operator_utilities import nearest_loc\n",
    "from TROPOMI_operator import apply_tropomi_operator\n",
    "from utils import save_obj, load_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of all TROPOMI_files that interesct our time period of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_of_interest = np.datetime64(datetime.datetime.strptime(config[\"startdate\"], \"%Y%m%dT%H%M%S\"))\n",
    "end_time_of_interest = np.datetime64(datetime.datetime.strptime(config[\"enddate\"], \"%Y%m%dT%H%M%S\"))\n",
    "\n",
    "def get_TROPOMI_times(filename):\n",
    "    file_times = re.search(r'(\\d{8}T\\d{6})_(\\d{8}T\\d{6})', filename)\n",
    "    start_TROPOMI_time = np.datetime64(datetime.datetime.strptime(file_times.group(1), \"%Y%m%dT%H%M%S\"))\n",
    "    end_TROPOMI_time = np.datetime64(datetime.datetime.strptime(file_times.group(2), \"%Y%m%dT%H%M%S\"))\n",
    "    return start_TROPOMI_time, end_TROPOMI_time\n",
    "    \n",
    "TROPOMI_files = [file for file in glob.glob(os.path.join(config[\"tropomi_cache\"], \"*.nc\"))\n",
    "                 if (start_time_of_interest <= get_TROPOMI_times(file)[0] <= end_time_of_interest)\n",
    "                 and (start_time_of_interest <= get_TROPOMI_times(file)[1] <= end_time_of_interest)]\n",
    "\n",
    "TROPOMI_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get what TROPOMI would have seen looking at GEOS-Chem and save one pkl file per TROPOMI file (i.e., Step1_convert_GC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tropomi_operator_to_one_tropomi_file(TROPOMI_file):\n",
    "    \n",
    "    result = apply_tropomi_operator(\n",
    "        filename = TROPOMI_file,\n",
    "        n_elements = False,\n",
    "        gc_startdate = start_time_of_interest,\n",
    "        gc_enddate = end_time_of_interest,\n",
    "        xlim = [-180, 180],\n",
    "        ylim = [-90, 90],\n",
    "        gc_cache = config[\"gccache\"],\n",
    "        build_jacobian = False,\n",
    "        sensi_cache = False)\n",
    "    \n",
    "    save_obj(result, os.path.join(config[\"workdir\"], \"step1\", os.path.basename(TROPOMI_file).replace(\".nc\",\"_GCtoTROPOMI.pkl\")))\n",
    "\n",
    "# Run the function across as many cores as you have\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "    pool.map(apply_tropomi_operator_to_one_tropomi_file, TROPOMI_files)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regrid the TROPOMI and GEOS-Chem data to daily averages on the 4 x 5 grid (i.e., Step2_regrid_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read any of the GEOS-Chem files to get the lat/lon grid\n",
    "with xr.open_dataset(glob.glob(os.path.join(config[\"gccache\"], \"GEOSChem.SpeciesConc*.nc4\"))[0]) as data:\n",
    "    LON = data[\"lon\"].values\n",
    "    LAT = data[\"lat\"].values\n",
    "    \n",
    "# List of all days in our time range of interest\n",
    "alldates = np.arange(start_time_of_interest, end_time_of_interest + np.timedelta64(1, 'D'), dtype='datetime64[D]')\n",
    "alldates = [day.astype(datetime.datetime).strftime(\"%Y%m%d\") for day in alldates]\n",
    "\n",
    "# Initialize arrays for regridding\n",
    "daily_TROPOMI = np.zeros((len(LON), len(LAT), len(alldates)))\n",
    "daily_GC = np.zeros((len(LON), len(LAT), len(alldates)))\n",
    "daily_count = np.zeros((len(LON), len(LAT), len(alldates)))\n",
    "\n",
    "# List of files from step 1 to get data to regrid from\n",
    "files_from_step1 = glob.glob(os.path.join(config[\"workdir\"], \"step1\", \"*.pkl\"))\n",
    "files_from_step1.sort()\n",
    "\n",
    "# Perform regridding\n",
    "for file in files_from_step1:\n",
    "    obs_GC = load_obj(file)[\"obs_GC\"]\n",
    "    NN = obs_GC.shape[0]\n",
    "    if NN == 0:\n",
    "        continue\n",
    "    \n",
    "    # For each TROPOMI obsevation, assign it to a GEOS-Chem grid cell\n",
    "    for iNN in range(NN):\n",
    "        \n",
    "        # Which day are we on (this is not perfect right now because orbits can cross from one day to the next...\n",
    "        # but it is the best we can do right now without changing apply_tropomi_operator)\n",
    "        file_times = re.search(r'(\\d{8}T\\d{6})_(\\d{8}T\\d{6})', file)\n",
    "        date = datetime.datetime.strptime(file_times.group(1), \"%Y%m%dT%H%M%S\").strftime(\"%Y%m%d\")\n",
    "        time_ind = alldates.index(date)\n",
    "\n",
    "        c_TROPOMI, c_GC, lon0, lat0 = obs_GC[iNN, :4]\n",
    "        ii = nearest_loc(lon0, LON, tolerance=5)\n",
    "        jj = nearest_loc(lat0, LAT, tolerance=4)\n",
    "        daily_TROPOMI[ii, jj, time_ind] += c_TROPOMI\n",
    "        daily_GC[ii, jj, time_ind] += c_GC\n",
    "        daily_count[ii, jj, time_ind] += 1\n",
    "\n",
    "# Normalize by how many observations got assigned to a grid cell to finish the regridding\n",
    "daily_count[daily_count == 0] = np.nan\n",
    "daily_TROPOMI = daily_TROPOMI / daily_count\n",
    "daily_GC = daily_GC / daily_count\n",
    "\n",
    "# Change order of dimensions\n",
    "regrid_CH4 = np.einsum(\"ijl->lji\", daily_TROPOMI) # (lon, lat, time) -> (time, lat, lon)\n",
    "regrid_GC = np.einsum(\"ijl->lji\", daily_GC) # (lon, lat, time) -> (time, lat, lon)\n",
    "\n",
    "# Write the netCDF file\n",
    "outputname = os.path.join(config[\"workdir\"], \"step2\", \"Daily_CH4.nc\")\n",
    "with Dataset(outputname, \"w\", format=\"NETCDF4_CLASSIC\") as dataset:\n",
    "    \n",
    "    lat = dataset.createDimension(\"lat\", len(LAT))\n",
    "    lon = dataset.createDimension(\"lon\", len(LON))\n",
    "    time = dataset.createDimension(\"time\", len(alldates))\n",
    "    \n",
    "    latitudes = dataset.createVariable(\"lat\", \"f8\", (\"lat\",))\n",
    "    longitudes = dataset.createVariable(\"lon\", \"f8\", (\"lon\",))\n",
    "    dates = dataset.createVariable(\"date\", \"i\", (\"time\",))\n",
    "    \n",
    "    nc_CH4 = dataset.createVariable(\"CH4\", \"f8\", (\"time\", \"lat\", \"lon\"))\n",
    "    nc_GC = dataset.createVariable(\"GC\", \"f8\", (\"time\", \"lat\", \"lon\"))\n",
    "    \n",
    "    latitudes[:] = LAT\n",
    "    longitudes[:] = LON\n",
    "    dates[:] = alldates\n",
    "    nc_CH4[:, :, :] = regrid_CH4\n",
    "    nc_GC[:, :, :] = regrid_GC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-imi_env]",
   "language": "python",
   "name": "conda-env-miniconda3-imi_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
