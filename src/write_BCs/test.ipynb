{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "write_boundary_conditions.py output for blendedTROPOMI=False\n",
      "Using files at /n/holylfs05/LABS/jacob_lab/Lab/imi/ch4/tropomi\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import yaml\n",
    "with open(\"config_boundary_conditions.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "sys.path.insert(0, \"../../\")\n",
    "from src.inversion_scripts.operators.operator_utilities import nearest_loc\n",
    "from src.inversion_scripts.operators.TROPOMI_operator import apply_tropomi_operator\n",
    "from src.inversion_scripts.utils import save_obj, load_obj\n",
    "\n",
    "blendedTROPOMI = False#(sys.argv[1] == \"True\")\n",
    "satelliteDir = \"/n/holylfs05/LABS/jacob_lab/Lab/imi/ch4/tropomi\"#sys.argv[2]\n",
    "print(f\"\\nwrite_boundary_conditions.py output for blendedTROPOMI={blendedTROPOMI}\")\n",
    "print(f\"Using files at {satelliteDir}\")\n",
    "\n",
    "\"\"\"\n",
    "This script works in three parts, utilizing the GEOS-Chem output from run_boundary_conditions.sh\n",
    "(1) Make a gridded (2.0 x 2.5 x daily) field of TROPOMI/GEOS-Chem co-locations\n",
    "    - for every TROPOMI observation, apply the TROPOMI operator to the GEOS-Chem fields\n",
    "    - average the TROPOMI XCH4 and GEOS-Chem XCH4 to a (2.0 x 2.5) grid for each day\n",
    "(2) Make a gridded (2.0 x 2.5 x daily) field of the bias between TROPOMI and GEOS-Chem\n",
    "    - subtract the TROPOMI and GEOS-Chem grids from part 1 to get a starting point for the bias\n",
    "    - smooth this field spatially (5 lon grid boxes, 5 lat grid boxes) then temporally (15 days backwards)\n",
    "    - fill NaN values with the latitudinal average at that time\n",
    "        - for a latitudinal average to be defined, there must be >= 30 grid cells at that latitude\n",
    "        - when a latitudinal average cannot be found, the closest latitudinal average is used\n",
    "(3) Write the boundary conditions\n",
    "    - using the bias from Part 2, subtract the (GC-TROPOMI) bias from the GC boundary conditions\n",
    "\"\"\"\n",
    "\n",
    "def get_TROPOMI_times(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that parses the TROPOMI filenames to get the start and end times.\n",
    "    Example input (str): S5P_RPRO_L2__CH4____20220725T152751_20220725T170921_24775_03_020400_20230201T100624.nc\n",
    "    Example output (tuple): (np.datetime64('2022-07-25T15:27:51'), np.datetime64('2022-07-25T17:09:21'))\n",
    "    \"\"\"\n",
    "\n",
    "    file_times = re.search(r'(\\d{8}T\\d{6})_(\\d{8}T\\d{6})', filename)\n",
    "    assert file_times is not None, \"check TROPOMI filename - wasn't able to find start and end times in the filename\"\n",
    "    start_TROPOMI_time = np.datetime64(datetime.datetime.strptime(file_times.group(1), \"%Y%m%dT%H%M%S\"))\n",
    "    end_TROPOMI_time = np.datetime64(datetime.datetime.strptime(file_times.group(2), \"%Y%m%dT%H%M%S\"))\n",
    "    \n",
    "    return start_TROPOMI_time, end_TROPOMI_time\n",
    "\n",
    "def apply_tropomi_operator_to_one_tropomi_file(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Run apply_tropomi_operator from src/inversion_scripts/operators/TROPOMI_operator.py for a single TROPOMI file (then saves it to a pkl file)\n",
    "    Example input (str): S5P_RPRO_L2__CH4____20220725T152751_20220725T170921_24775_03_020400_20230201T100624.nc\n",
    "    Example output: write the file config[\"workdir\"]/step1/S5P_RPRO_L2__CH4____20220725T152751_20220725T170921_24775_03_020400_20230201T100624_GCtoTROPOMI.pkl\n",
    "    \"\"\"\n",
    "    \n",
    "    result = apply_tropomi_operator(\n",
    "        filename = filename,\n",
    "        BlendedTROPOMI = blendedTROPOMI,\n",
    "        n_elements = False, # Not relevant\n",
    "        gc_startdate = start_time_of_interest,\n",
    "        gc_enddate = end_time_of_interest,\n",
    "        xlim = [-180, 180],\n",
    "        ylim = [-90, 90],\n",
    "        gc_cache = os.path.join(config[\"workDir\"], \"gc_run\", \"OutputDir\"),\n",
    "        build_jacobian = False, # Not relevant\n",
    "        sensi_cache = False) # Not relevant\n",
    "    \n",
    "    return result[\"obs_GC\"],filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First TROPOMI file -> /n/holylfs05/LABS/jacob_lab/Lab/imi/ch4/tropomi/S5P_RPRO_L2__CH4____20180430T001950_20180430T020120_02818_03_020400_20221107T155110.nc\n",
      "Last TROPOMI file  -> /n/holylfs05/LABS/jacob_lab/Lab/imi/ch4/tropomi/S5P_RPRO_L2__CH4____20180530T211722_20180530T225852_03256_03_020400_20221110T091912.nc\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2219 is out of bounds for axis 1 with size 215",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/tmp/ipykernel_1595350/3690373344.py\", line 64, in apply_tropomi_operator_to_one_tropomi_file\n  File \"../../src/inversion_scripts/operators/TROPOMI_operator.py\", line 261, in apply_tropomi_operator\n    time = pd.to_datetime(str(TROPOMI[\"time\"][iSat,jSat]))\nIndexError: index 2219 is out of bounds for axis 1 with size 215\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLast TROPOMI file  -> \u001b[39m\u001b[39m{\u001b[39;00mTROPOMI_files[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Using as many cores as you have, apply the TROPOMI operator to each file\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m obsGC_and_filenames \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)(delayed(apply_tropomi_operator_to_one_tropomi_file)(filename) \u001b[39mfor\u001b[39;49;00m filename \u001b[39min\u001b[39;49;00m TROPOMI_files)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Read any of the GEOS-Chem files to get the lat/lon grid\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mwith\u001b[39;00m xr\u001b[39m.\u001b[39mopen_dataset(glob\u001b[39m.\u001b[39mglob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(config[\u001b[39m\"\u001b[39m\u001b[39mworkDir\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mgc_run\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mOutputDir\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGEOSChem.SpeciesConc*.nc4\u001b[39m\u001b[39m\"\u001b[39m))[\u001b[39m0\u001b[39m]) \u001b[39mas\u001b[39;00m data:\n",
      "File \u001b[0;32m/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/site-packages/joblib/parallel.py:1054\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1054\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1055\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/site-packages/joblib/parallel.py:933\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 933\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    934\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/concurrent/futures/_base.py:440\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    439\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/n/holylfs05/LABS/jacob_lab/Users/nbalasus/miniconda3/envs/imi_env/lib/python3.9/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_result\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2219 is out of bounds for axis 1 with size 215"
     ]
    }
   ],
   "source": [
    "### Part 1 ###\n",
    "\n",
    "# From config file, get the start and end times that we will be writing boundary conditions for (+20 days on the end because of our temporal smoothing)\n",
    "start_time_of_interest = np.datetime64(datetime.datetime.strptime(config[\"startDate\"], \"%Y%m%d\"))\n",
    "end_time_of_interest = np.datetime64(datetime.datetime.strptime(config[\"endDate\"], \"%Y%m%d\"))\n",
    "\n",
    "# List of all TROPOMI files that interesct our time period of interest\n",
    "TROPOMI_files = sorted([file for file in glob.glob(os.path.join(satelliteDir, \"*.nc\"))\n",
    "                        if (start_time_of_interest <= get_TROPOMI_times(file)[0] <= end_time_of_interest)\n",
    "                        and (start_time_of_interest <= get_TROPOMI_times(file)[1] <= end_time_of_interest)])\n",
    "print(f\"First TROPOMI file -> {TROPOMI_files[0]}\")\n",
    "print(f\"Last TROPOMI file  -> {TROPOMI_files[-1]}\")\n",
    "\n",
    "# Using as many cores as you have, apply the TROPOMI operator to each file\n",
    "obsGC_and_filenames = Parallel(n_jobs=-1)(delayed(apply_tropomi_operator_to_one_tropomi_file)(filename) for filename in TROPOMI_files)\n",
    "\n",
    "# Read any of the GEOS-Chem files to get the lat/lon grid\n",
    "with xr.open_dataset(glob.glob(os.path.join(config[\"workDir\"], \"gc_run\", \"OutputDir\", \"GEOSChem.SpeciesConc*.nc4\"))[0]) as data:\n",
    "    LON = data[\"lon\"].values\n",
    "    LAT = data[\"lat\"].values\n",
    "\n",
    "# List of all days in our time range of interest\n",
    "alldates = np.arange(start_time_of_interest, end_time_of_interest + np.timedelta64(1, 'D'), dtype='datetime64[D]')\n",
    "alldates = [day.astype(datetime.datetime).strftime(\"%Y%m%d\") for day in alldates]\n",
    "\n",
    "# Initialize arrays for regridding\n",
    "daily_TROPOMI = np.zeros((len(LON), len(LAT), len(alldates)))\n",
    "daily_GC = np.zeros((len(LON), len(LAT), len(alldates)))\n",
    "daily_count = np.zeros((len(LON), len(LAT), len(alldates)))\n",
    "\n",
    "# Loop thorugh all of the files which now contain TROPOMI and the corresponding GC XCH4\n",
    "for obsGC,filename in obsGC_and_filenames:\n",
    "    NN = obsGC.shape[0]\n",
    "    if NN == 0:\n",
    "        continue\n",
    "\n",
    "    # For each TROPOMI observation, assign it to a GEOS-Chem grid cell\n",
    "    for iNN in range(NN):\n",
    "            \n",
    "            # Which day are we on (this is not perfect right now because orbits can cross from one day to the next...\n",
    "            # but it is the best we can do right now without changing apply_tropomi_operator)\n",
    "            file_times = re.search(r'(\\d{8}T\\d{6})_(\\d{8}T\\d{6})', filename)\n",
    "            assert file_times is not None, \"check TROPOMI filename - wasn't able to find start and end times in the filename\"\n",
    "            date = datetime.datetime.strptime(file_times.group(1), \"%Y%m%dT%H%M%S\").strftime(\"%Y%m%d\")\n",
    "            time_ind = alldates.index(date)\n",
    "\n",
    "            c_TROPOMI, c_GC, lon0, lat0 = obsGC[iNN, :4]\n",
    "            ii = nearest_loc(lon0, LON, tolerance=5)\n",
    "            jj = nearest_loc(lat0, LAT, tolerance=4)\n",
    "            daily_TROPOMI[ii, jj, time_ind] += c_TROPOMI\n",
    "            daily_GC[ii, jj, time_ind] += c_GC\n",
    "            daily_count[ii, jj, time_ind] += 1\n",
    "\n",
    "# Normalize by how many observations got assigned to a grid cell to finish the regridding\n",
    "daily_count[daily_count == 0] = np.nan\n",
    "daily_TROPOMI = daily_TROPOMI / daily_count\n",
    "daily_GC = daily_GC / daily_count\n",
    "\n",
    "# Change dimensions\n",
    "regrid_TROPOMI = np.einsum(\"ijl->lji\", daily_TROPOMI) # (lon, lat, time) -> (time, lat, lon)\n",
    "regrid_GC = np.einsum(\"ijl->lji\", daily_GC) # (lon, lat, time) -> (time, lat, lon)\n",
    "\n",
    "# Make a Dataset with variables of (TROPOMI_CH4, GC_CH4) and dims of (lon, lat, time)\n",
    "ds = xr.Dataset({\n",
    "    'TROPOMI_CH4': xr.DataArray(\n",
    "        data = regrid_TROPOMI,\n",
    "        dims = [\"time\", \"lat\", \"lon\"],\n",
    "        coords = {\"time\": alldates, \"lat\": LAT, \"lon\": LON}\n",
    "        ),\n",
    "    'GC_CH4': xr.DataArray(\n",
    "        data = regrid_GC,\n",
    "        dims = [\"time\", \"lat\", \"lon\"],\n",
    "        coords = {\"time\": alldates, \"lat\": LAT, \"lon\": LON}\n",
    "        ),\n",
    "})\n",
    "\n",
    "### Part 2 ###\n",
    "\n",
    "bias = ds[\"GC_CH4\"] - ds[\"TROPOMI_CH4\"]\n",
    "\n",
    "# Smooth spatially\n",
    "bias = bias.rolling(lat=5,              # five lat grid boxes (10 degrees)\n",
    "                    lon=5,              # five lon grid boxes (12.5 degrees)\n",
    "                    center=True,        # five boxes includes the one we are cented on\n",
    "                    min_periods=25/2    # half of the grid cells have a value to not output NaN\n",
    "                    ).mean(skipna=True)\n",
    "\n",
    "# Smooth temporally\n",
    "bias = bias.rolling(time=15,            # average 15 days back in time (including the time we are centered on)\n",
    "                    min_periods=1,      # only one of the time values must have a value to not output NaN\n",
    "                ).mean(skipna=True)\n",
    "\n",
    "# Create a dataarray with latitudinal average for each time step\n",
    "# We will fill the NaN values in bias with these averages\n",
    "nan_value_filler_2d = bias.copy()\n",
    "nan_value_filler_2d = (nan_value_filler_2d.where(nan_value_filler_2d.count(\"lon\") >= 30) # there needs to be 30 grid boxes       \n",
    "                                        .mean(dim=[\"lon\"], skipna=True)                #    at this lat to define a mean\n",
    "                                        .interpolate_na(dim=\"lat\", method=\"nearest\")   # fill in \"middle\" NaN values\n",
    "                                        .bfill(dim=\"lat\")                              # fill in NaN values towards -90 deg\n",
    "                                        .ffill(dim=\"lat\")                              # fill in NaN values towards +90 deg\n",
    "                                    )\n",
    "\n",
    "# Expand to 3 dimensions\n",
    "nan_value_filler_3d = bias.copy() * np.nan\n",
    "for i in range(len(LON)):\n",
    "    nan_value_filler_3d[:,:,i] = nan_value_filler_2d\n",
    "\n",
    "# Use these values to fill NaNs\n",
    "bias = bias.fillna(nan_value_filler_3d)\n",
    "\n",
    "print(f\"Average bias (GC-TROPOMI): {bias.mean().values:.2f} ppb\\n\")\n",
    "\n",
    "# If there are still NaNs (this will happen when TROPOMI data is missing), use 0.0 ppb as the bias but warn the user\n",
    "for t in range(len(bias[\"time\"].values)):\n",
    "    if np.any(np.isnan(bias[t,:,:].values)):\n",
    "        print(f\"WARNING -> using 0.0 ppb as bias for {bias['time'].values[t]}\")\n",
    "        bias[t,:,:] = bias[t,:,:].fillna(0)\n",
    "\n",
    "### Part 3 ###\n",
    "\n",
    "# Get dates and convert the total column bias to mol/mol\n",
    "strdate = bias[\"time\"].values\n",
    "bias = bias.values * 1e-9\n",
    "\n",
    "# Only write BCs for our date range\n",
    "files = sorted(glob.glob(os.path.join(config[\"workDir\"], \"gc_run\", \"OutputDir\", \"GEOSChem.BoundaryConditions*.nc4\")))\n",
    "files = [\n",
    "    f\n",
    "    for f in files\n",
    "    if (\n",
    "        (np.datetime64(re.search(r'(\\d{8})_(\\d{4}z)', f).group(1)) >= np.datetime64(config[\"startDate\"]))\n",
    "        & (np.datetime64(re.search(r'(\\d{8})_(\\d{4}z)', f).group(1)) <= np.datetime64(config[\"endDate\"]))\n",
    "    )\n",
    "]\n",
    "\n",
    "# For each file, remove the total column bias from each level of the GEOS-Chem boundary condition\n",
    "for filename in files:\n",
    "    l = [index for index,date in enumerate(strdate) if date == re.search(r'(\\d{8})_(\\d{4}z)', filename).group(1)]\n",
    "    assert len(l) == 1, \"ERROR -> there should only be bias per boundary condition file\"\n",
    "    index = l[0]\n",
    "    bias_for_this_boundary_condition_file = bias[index, :, :]\n",
    "\n",
    "    with xr.open_dataset(filename) as ds:\n",
    "        original_data = ds[\"SpeciesBC_CH4\"].values.copy()\n",
    "        for t in range(original_data.shape[0]):\n",
    "            for lev in range(original_data.shape[1]):\n",
    "                original_data[t, lev, :, :] -= bias_for_this_boundary_condition_file\n",
    "        ds[\"SpeciesBC_CH4\"].values = original_data\n",
    "        if blendedTROPOMI:\n",
    "            print(f\"Writing to {os.path.join(config['workDir'], 'blended-boundary-conditions', os.path.basename(filename))}\")\n",
    "            ds.to_netcdf(os.path.join(config[\"workDir\"], \"blended-boundary-conditions\", os.path.basename(filename)))\n",
    "        else:\n",
    "            print(f\"Writing to {os.path.join(config['workDir'], 'tropomi-boundary-conditions', os.path.basename(filename))}\")\n",
    "            ds.to_netcdf(os.path.join(config[\"workDir\"], \"tropomi-boundary-conditions\", os.path.basename(filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2219 is out of bounds for axis 1 with size 215",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m apply_tropomi_operator_to_one_tropomi_file(TROPOMI_files[\u001b[39m0\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m, in \u001b[0;36mapply_tropomi_operator_to_one_tropomi_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_tropomi_operator_to_one_tropomi_file\u001b[39m(filename):\n\u001b[1;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m    Run apply_tropomi_operator from src/inversion_scripts/operators/TROPOMI_operator.py for a single TROPOMI file (then saves it to a pkl file)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m    Example input (str): S5P_RPRO_L2__CH4____20220725T152751_20220725T170921_24775_03_020400_20230201T100624.nc\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m    Example output: write the file config[\"workdir\"]/step1/S5P_RPRO_L2__CH4____20220725T152751_20220725T170921_24775_03_020400_20230201T100624_GCtoTROPOMI.pkl\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     result \u001b[39m=\u001b[39m apply_tropomi_operator(\n\u001b[1;32m     65\u001b[0m         filename \u001b[39m=\u001b[39;49m filename,\n\u001b[1;32m     66\u001b[0m         BlendedTROPOMI \u001b[39m=\u001b[39;49m blendedTROPOMI,\n\u001b[1;32m     67\u001b[0m         n_elements \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, \u001b[39m# Not relevant\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m         gc_startdate \u001b[39m=\u001b[39;49m start_time_of_interest,\n\u001b[1;32m     69\u001b[0m         gc_enddate \u001b[39m=\u001b[39;49m end_time_of_interest,\n\u001b[1;32m     70\u001b[0m         xlim \u001b[39m=\u001b[39;49m [\u001b[39m-\u001b[39;49m\u001b[39m180\u001b[39;49m, \u001b[39m180\u001b[39;49m],\n\u001b[1;32m     71\u001b[0m         ylim \u001b[39m=\u001b[39;49m [\u001b[39m-\u001b[39;49m\u001b[39m90\u001b[39;49m, \u001b[39m90\u001b[39;49m],\n\u001b[1;32m     72\u001b[0m         gc_cache \u001b[39m=\u001b[39;49m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(config[\u001b[39m\"\u001b[39;49m\u001b[39mworkDir\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m\"\u001b[39;49m\u001b[39mgc_run\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mOutputDir\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     73\u001b[0m         build_jacobian \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, \u001b[39m# Not relevant\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m         sensi_cache \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Not relevant\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mobs_GC\u001b[39m\u001b[39m\"\u001b[39m],filename\n",
      "File \u001b[0;32m~/imi-v2023-08-bcs/integrated_methane_inversion/src/inversion_scripts/operators/TROPOMI_operator.py:261\u001b[0m, in \u001b[0;36mapply_tropomi_operator\u001b[0;34m(filename, BlendedTROPOMI, n_elements, gc_startdate, gc_enddate, xlim, ylim, gc_cache, build_jacobian, sensi_cache)\u001b[0m\n\u001b[1;32m    259\u001b[0m iSat \u001b[39m=\u001b[39m sat_ind[\u001b[39m0\u001b[39m][k]  \u001b[39m# lat index\u001b[39;00m\n\u001b[1;32m    260\u001b[0m jSat \u001b[39m=\u001b[39m sat_ind[\u001b[39m1\u001b[39m][k]  \u001b[39m# lon index\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m time \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(\u001b[39mstr\u001b[39m(TROPOMI[\u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m][iSat,jSat]))\n\u001b[1;32m    262\u001b[0m strdate \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mround(\u001b[39m\"\u001b[39m\u001b[39m60min\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m all_strdate\u001b[39m.\u001b[39mappend(strdate)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2219 is out of bounds for axis 1 with size 215"
     ]
    }
   ],
   "source": [
    "apply_tropomi_operator_to_one_tropomi_file(TROPOMI_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = TROPOMI_files[0]\n",
    "BlendedTROPOMI = blendedTROPOMI\n",
    "n_elements = False # Not relevant\n",
    "gc_startdate = start_time_of_interest\n",
    "gc_enddate = end_time_of_interest\n",
    "xlim = [-180, 180]\n",
    "ylim = [-90, 90]\n",
    "gc_cache = os.path.join(config[\"workDir\"], \"gc_run\", \"OutputDir\")\n",
    "build_jacobian = False # Not relevant\n",
    "sensi_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from shapely.geometry import Polygon\n",
    "from src.inversion_scripts.utils import (\n",
    "    filter_tropomi,\n",
    "    filter_blended,\n",
    ")\n",
    "from src.inversion_scripts.operators.operator_utilities import (\n",
    "    get_gc_lat_lon,\n",
    "    read_all_geoschem,\n",
    "    merge_pressure_grids,\n",
    "    remap,\n",
    "    remap_sensitivities,\n",
    "    get_gridcell_list,\n",
    "    nearest_loc,\n",
    ")\n",
    "\n",
    "def read_tropomi(filename):\n",
    "    \"\"\"\n",
    "    Read TROPOMI data and save important variables to dictionary.\n",
    "\n",
    "    Arguments\n",
    "        filename [str]  : TROPOMI netcdf data file to read\n",
    "\n",
    "    Returns\n",
    "        dat      [dict] : Dictionary of important variables from TROPOMI:\n",
    "                            - CH4\n",
    "                            - Latitude\n",
    "                            - Longitude\n",
    "                            - QA value\n",
    "                            - UTC time\n",
    "                            - Time (utc time reshaped for orbit)\n",
    "                            - Averaging kernel\n",
    "                            - SWIR albedo\n",
    "                            - NIR albedo\n",
    "                            - Blended albedo\n",
    "                            - CH4 prior profile\n",
    "                            - Dry air subcolumns\n",
    "                            - Latitude bounds\n",
    "                            - Longitude bounds\n",
    "                            - Vertical pressure profile\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionary for TROPOMI data\n",
    "    dat = {}\n",
    "\n",
    "    # Catch read errors in any of the variables\n",
    "    try:\n",
    "        # Store methane, QA, lat, lon, and time\n",
    "        with xr.open_dataset(filename, group=\"PRODUCT\") as tropomi_data:\n",
    "            dat[\"methane\"] = tropomi_data[\"methane_mixing_ratio_bias_corrected\"].values[0, :, :]\n",
    "            dat[\"qa_value\"] = tropomi_data[\"qa_value\"].values[0, :, :]\n",
    "            dat[\"longitude\"] = tropomi_data[\"longitude\"].values[0, :, :]\n",
    "            dat[\"latitude\"] = tropomi_data[\"latitude\"].values[0, :, :]\n",
    "\n",
    "            utc_str = tropomi_data[\"time_utc\"].values[0,:]\n",
    "            utc_str = np.array([d.replace(\"Z\",\"\") for d in utc_str]).astype(\"datetime64[ns]\")\n",
    "            dat[\"time\"] = np.repeat(utc_str[:, np.newaxis], dat[\"methane\"].shape[1], axis=1)\n",
    "\n",
    "        # Store column averaging kernel, SWIR and NIR surface albedo\n",
    "        with xr.open_dataset(filename, group=\"PRODUCT/SUPPORT_DATA/DETAILED_RESULTS\") as tropomi_data:\n",
    "            dat[\"column_AK\"] = tropomi_data[\"column_averaging_kernel\"].values[0, :, :, ::-1]\n",
    "            dat[\"swir_albedo\"] = tropomi_data[\"surface_albedo_SWIR\"].values[0, :, :]\n",
    "            dat[\"nir_albedo\"] = tropomi_data[\"surface_albedo_NIR\"].values[0, :, :]\n",
    "            dat[\"blended_albedo\"] = 2.4 * dat[\"nir_albedo\"] - 1.13 * dat[\"swir_albedo\"]\n",
    "\n",
    "        # Store methane prior profile, dry air subcolumns\n",
    "        with xr.open_dataset(filename, group=\"PRODUCT/SUPPORT_DATA/INPUT_DATA\") as tropomi_data:\n",
    "            dat[\"methane_profile_apriori\"] = tropomi_data[\"methane_profile_apriori\"].values[0, :, :, ::-1]  # mol m-2\n",
    "            dat[\"dry_air_subcolumns\"] = tropomi_data[\"dry_air_subcolumns\"].values[0, :, :, ::-1]  # mol m-2\n",
    "            dat[\"surface_classification\"] = (tropomi_data[\"surface_classification\"].values[:].astype(\"uint8\") & 0x03).astype(int)\n",
    "\n",
    "            # Also get pressure interval and surface pressure for use below\n",
    "            pressure_interval = (tropomi_data[\"pressure_interval\"].values[0, :, :] / 100)  # Pa -> hPa\n",
    "            surface_pressure = (tropomi_data[\"surface_pressure\"].values[0, :, :] / 100)  # Pa -> hPa\n",
    "\n",
    "        # Store latitude and longitude bounds for pixels\n",
    "        with xr.open_dataset(filename, group=\"PRODUCT/SUPPORT_DATA/GEOLOCATIONS\") as tropomi_data:\n",
    "            dat[\"longitude_bounds\"] = tropomi_data[\"longitude_bounds\"].values[0, :, :, :]\n",
    "            dat[\"latitude_bounds\"] = tropomi_data[\"latitude_bounds\"].values[0, :, :, :]\n",
    "\n",
    "        # Store vertical pressure profile\n",
    "        n1 = dat[\"methane\"].shape[0]  # length of along-track dimension (scanline) of retrieval field\n",
    "        n2 = dat[\"methane\"].shape[1]  # length of across-track dimension (ground_pixel) of retrieval field\n",
    "        pressures = np.full([n1, n2, 12 + 1], np.nan, dtype=np.float32)\n",
    "        for i in range(12 + 1):\n",
    "            pressures[:, :, i] = surface_pressure - i * pressure_interval\n",
    "        dat[\"pressures\"] = pressures\n",
    "\n",
    "    # Return an error if any of the variables were not read correctly\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return dat\n",
    "\n",
    "def read_blended(filename):\n",
    "    \"\"\"\n",
    "    Read Blended TROPOMI+GOSAT data and save important variables to dictionary.\n",
    "    Arguments\n",
    "        filename [str]  : Blended TROPOMI+GOSAT netcdf data file to read\n",
    "    Returns\n",
    "        dat      [dict] : Dictionary of important variables from Blended TROPOMI+GOSAT:\n",
    "                            - CH4\n",
    "                            - Latitude\n",
    "                            - Longitude\n",
    "                            - Time (utc time reshaped for orbit)\n",
    "                            - Averaging kernel\n",
    "                            - SWIR albedo\n",
    "                            - NIR albedo\n",
    "                            - Blended albedo\n",
    "                            - CH4 prior profile\n",
    "                            - Dry air subcolumns\n",
    "                            - Latitude bounds\n",
    "                            - Longitude bounds\n",
    "                            - Surface classification\n",
    "                            - Chi-Square for SWIR\n",
    "                            - Vertical pressure profile\n",
    "    \"\"\"\n",
    "    assert \"BLND\" in filename, f\"BLND not in filename {filename}, but a blended function is being used\"\n",
    "\n",
    "    try:\n",
    "        # Initialize dictionary for Blended TROPOMI+GOSAT data\n",
    "        dat = {}\n",
    "\n",
    "        # Extract data from netCDF file to our dictionary\n",
    "        with xr.open_dataset(filename) as blended_data:\n",
    "\n",
    "            dat[\"methane\"] = blended_data[\"methane_mixing_ratio_blended\"].values[:]\n",
    "            dat[\"longitude\"] = blended_data[\"longitude\"].values[:]\n",
    "            dat[\"latitude\"] = blended_data[\"latitude\"].values[:]\n",
    "            dat[\"column_AK\"] = blended_data[\"column_averaging_kernel\"].values[:, ::-1]\n",
    "            dat[\"swir_albedo\"] = blended_data[\"surface_albedo_SWIR\"][:]\n",
    "            dat[\"nir_albedo\"] = blended_data[\"surface_albedo_NIR\"].values[:]\n",
    "            dat[\"blended_albedo\"] = 2.4 * dat[\"nir_albedo\"] - 1.13 * dat[\"swir_albedo\"]\n",
    "            dat[\"methane_profile_apriori\"] = blended_data[\"methane_profile_apriori\"].values[:, ::-1]\n",
    "            dat[\"dry_air_subcolumns\"] = blended_data[\"dry_air_subcolumns\"].values[:, ::-1]\n",
    "            dat[\"longitude_bounds\"] = blended_data[\"longitude_bounds\"].values[:]\n",
    "            dat[\"latitude_bounds\"] = blended_data[\"latitude_bounds\"].values[:]\n",
    "            dat[\"surface_classification\"] = (blended_data[\"surface_classification\"].values[:].astype(\"uint8\") & 0x03).astype(int)\n",
    "            dat[\"chi_square_SWIR\"] = blended_data[\"chi_square_SWIR\"].values[:]\n",
    "\n",
    "            # Remove \"Z\" from time so that numpy doesn't throw a warning\n",
    "            utc_str = blended_data[\"time_utc\"].values[:]\n",
    "            dat[\"time\"] = np.array([d.replace(\"Z\",\"\") for d in utc_str]).astype(\"datetime64[ns]\")\n",
    "\n",
    "            # Need to calculate the pressure for the 13 TROPOMI levels (12 layer edges)\n",
    "            pressure_interval = (blended_data[\"pressure_interval\"].values[:] / 100)  # Pa -> hPa\n",
    "            surface_pressure = (blended_data[\"surface_pressure\"].values[:] / 100)    # Pa -> hPa\n",
    "            n = len(dat[\"methane\"])\n",
    "            pressures = np.full([n, 12 + 1], np.nan, dtype=np.float32)\n",
    "            for i in range(12 + 1):\n",
    "                pressures[:, i] = surface_pressure - i * pressure_interval\n",
    "            dat[\"pressures\"] = pressures\n",
    "\n",
    "        # Add an axis here to mimic the (scanline, groundpixel) format of operational TROPOMI data\n",
    "        # This is so the blended data will be compatible with the TROPOMI operators\n",
    "        for key in dat.keys():\n",
    "            dat[key] = np.expand_dims(dat[key], axis=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2219 is out of bounds for axis 1 with size 215",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m iSat \u001b[39m=\u001b[39m sat_ind[\u001b[39m0\u001b[39m][k]  \u001b[39m# lat index\u001b[39;00m\n\u001b[1;32m     35\u001b[0m jSat \u001b[39m=\u001b[39m sat_ind[\u001b[39m1\u001b[39m][k]  \u001b[39m# lon index\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m time \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(\u001b[39mstr\u001b[39m(TROPOMI[\u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m][iSat,jSat]))\n\u001b[1;32m     37\u001b[0m strdate \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mround(\u001b[39m\"\u001b[39m\u001b[39m60min\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m all_strdate\u001b[39m.\u001b[39mappend(strdate)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2219 is out of bounds for axis 1 with size 215"
     ]
    }
   ],
   "source": [
    "# Read TROPOMI data\n",
    "assert isinstance(BlendedTROPOMI, bool), \"BlendedTROPOMI is not a bool\"\n",
    "if BlendedTROPOMI:\n",
    "    TROPOMI = read_blended(filename)\n",
    "else:\n",
    "    TROPOMI = read_tropomi(filename)\n",
    "if TROPOMI == None:\n",
    "    print(f\"Skipping {filename} due to file processing issue.\")\n",
    "    print(\"ERROR\")\n",
    "\n",
    "if BlendedTROPOMI:\n",
    "    # Only going to consider blended data within lat/lon/time bounds and wihtout problematic coastal pixels\n",
    "    sat_ind = filter_blended(TROPOMI, xlim, ylim, gc_startdate, gc_enddate)\n",
    "else:\n",
    "    # Only going to consider TROPOMI data within lat/lon/time bounds and with QA > 0.5\n",
    "    sat_ind = filter_tropomi(TROPOMI, xlim, ylim, gc_startdate, gc_enddate)\n",
    "\n",
    "# Number of TROPOMI observations\n",
    "n_obs = len(sat_ind[0])\n",
    "# print(\"Found\", n_obs, \"TROPOMI observations.\")\n",
    "\n",
    "# If need to build Jacobian from GEOS-Chem perturbation simulation sensitivity data:\n",
    "if build_jacobian:\n",
    "    # Initialize Jacobian K\n",
    "    jacobian_K = np.zeros([n_obs, n_elements], dtype=np.float32)\n",
    "    jacobian_K.fill(np.nan)\n",
    "\n",
    "# Initialize a list to store the dates we want to look at\n",
    "all_strdate = []\n",
    "\n",
    "# For each TROPOMI observation\n",
    "for k in range(n_obs):\n",
    "    # Get the date and hour\n",
    "    iSat = sat_ind[0][k]  # lat index\n",
    "    jSat = sat_ind[1][k]  # lon index\n",
    "    time = pd.to_datetime(str(TROPOMI[\"time\"][iSat,jSat]))\n",
    "    strdate = time.round(\"60min\").strftime(\"%Y%m%d_%H\")\n",
    "    all_strdate.append(strdate)\n",
    "all_strdate = list(set(all_strdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_ind = filter_tropomi(TROPOMI, xlim, ylim, gc_startdate, gc_enddate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2906, 215)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TROPOMI[\"surface_classification\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2906, 215)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TROPOMI[\"methane\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
